#defaults:
#  - override hydra/launcher: joblib

hydra:
  job:
    chdir: false

target:
  aug_global_centering: false
  aug_scale: 1.0

flow:
  base:
    aug_scale_init: ${target.aug_scale}
    double_centrered_gaussian: false
  dim: 3
  nodes: 9
  n_layers: 8
  identity_init: true
  type: proj  # vector_scale_shift  vector_scale nice proj
  kwargs:
    proj:
      global_frame: false
      process_flow_params_jointly: false
      condition_on_x_proj: true
      gram_schmidt: false
      mlp_function_units: [16,]
    proj_v2:
      process_flow_params_jointly: false
      condition_on_x_proj: true
      gram_schmidt: false
      mlp_function_units: [16,]
      n_vectors: 10
  act_norm: false
  fast_compile: true
  compile_n_unroll: 1
  mace:
    n_invariant_feat_hidden: 16
    n_vectors_hidden: 16
    bessel_number: int  # Number of bessel functions.
    r_max: 5.
  egnn:
    mlp_units: [32,32]
    n_layers: 3
    normalize_by_norms: true
    variance_scaling_init: 0.001
    tanh: true
    agg: mean
    phi_x_max: 1.0
    normalization_constant: 1.0
    h_linear_softmax: true
    h_embedding_dim: 16
  transformer: # for proj_v2 flow.
    mlp_units: [32,32]
    num_heads: 6
    key_size: 16
    w_init_scale: 0.1
    n_layers: 3
  mlp_head_config:
    mlp_units: [ 32, 32 ]


training:
  lr: 1e-4
  use_64_bit: false
  optimizer_name: adam
  n_epoch: 100
  batch_size: 200
  plot_batch_size: 100
  max_global_norm: 5_000
  seed: 0
  reload_aug_per_epoch: true
  train_set_size: null
  test_set_size: null
  n_plots: 10
  n_checkpoints: 0
  n_eval: 12
  K_marginal_log_lik: 20
  save: true
  save_dir: qm9pos_results

logger:
#  list_logger: null
  wandb:
    name: qm9pos_no_h_${flow.type}_batchsize${training.batch_size}_lr${training.lr}_layers${flow.n_layers}
    project: fab
    entity: flow-ais-bootstrap
    tags: [qm9pos]

